---
title: "Projects List"
date: 2021-02-26T20:18:54+03:00
layout: page
---

## Projects


### Large Language models
## {class="content-block"}
- #### [GPT Neo](projects/gpt-neo/)
    - [![alt](../../images/art49.png)](projects/gpt-neo/)
    - GPT-Neo is the name of our codebase for transformer-based language models loosely styled around the GPT architecture. One of our goals is to use GPT-Neo to replicate a GPT-3 sized model and open source it to the public, for free. 
    - completed
- #### [GPT NeoX](projects/gpt-neox/)
    - [![alt](../../images/art50.png) ](projects/gpt-neox/)
    - An implementation of model parallel GPT-3-like models on GPUs, based on the DeepSpeed library. Designed to be able to train models in the hundreds of billions of parameters or larger.
    - in progress

## {class="content-block"}
- #### [The Pile](projects/pile/)
    - [![alt](../../images/art43.png)](projects/pile/)
    - The Pile is an **825 GiB** diverse, open source language modelling dataset consisting of data from 22 high quality sources. It is useful for both training and benchmarking large language models. The Pile is now complete! Check it out [here](projects/pile).
    - completed
- #### [OpenWebText2](projects/open-web-text2/)
    - [![alt](../../images/art4.png) ](projects/open-web-text2/)
    - OpenWebText2 is a dataset inspired by WebText, created by scraping URLs extracted from Reddit submissions up until April 2020 with a minimum score of 3 as a proxy for quality. It features content from multiple languages, document metadata, multiple dataset versions, and open source replication code.
        **Update**: OpenWebText2 is complete! Check it out [here](projects/open-web-text2/).
    - completed
<!-- 
## {class="content-block"}
- #### [Eval Harness](projects/eval-harness/)
    - [![alt](../../images/art32.png) ](projects/eval-harness/)
    - A framework for few-shot evaluation of autoregressive language models.
    - in progress -->


<!-- ### Multimodal

## {class="content-block"}
- #### [DALLE-mtf](projects/dalle-mtf)
    - [![alt](../../images/art54.png) ](projects/dalle-mtf/)
    - Open-AI's DALL-E for large scale training in mesh-tensorflow.
    - in progress -->

<!-- 
### Bio ML

## {class="content-block"}
- #### [Massively Parallelized NERF](projects/massively-parallelized-nerf)
    - [![alt](../../images/art9.png) ](projects/massively-parallelized-nerf/)
    - ...coming sooon
    - in progress
- #### [Equivariant Transformers](projects/en-equivariant-transformers)
    - [![alt](../../images/art25.png) ](projects/en-equivariant-transformers)
    - ...coming sooon
    - in progress

## {class="content-block"}
- #### [AlphaFold2](projects/alpha-fold2)
    - [![alt](../../images/art60.png) ](projects/alpha-fold2/)
    - ...coming sooon
    - in progress
 -->


